{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbdulrahmanAlshugaa/AbdulrahmanAlshugaa/blob/main/project_Arabic_Words_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#================Cell 1==================\n",
        "Download and decompress the form full_grams_cbow_300_twitter.zip that was trained on data previously in order to be available here during the code execution"
      ],
      "metadata": {
        "id": "y5c9EGtlpMDb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cjXpwiymRU9",
        "outputId": "b463dbbc-029b-41e8-9ac2-ce9834c9fcb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-13 09:16:41--  https://archive.org/download/full_grams_cbow_300_twitter/full_grams_cbow_300_twitter.zip\n",
            "Resolving archive.org (archive.org)... 207.241.224.2\n",
            "Connecting to archive.org (archive.org)|207.241.224.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ia802803.us.archive.org/15/items/full_grams_cbow_300_twitter/full_grams_cbow_300_twitter.zip [following]\n",
            "--2022-03-13 09:16:42--  https://ia802803.us.archive.org/15/items/full_grams_cbow_300_twitter/full_grams_cbow_300_twitter.zip\n",
            "Resolving ia802803.us.archive.org (ia802803.us.archive.org)... 207.241.232.113\n",
            "Connecting to ia802803.us.archive.org (ia802803.us.archive.org)|207.241.232.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3325529808 (3.1G) [application/zip]\n",
            "Saving to: ‘full_grams_cbow_300_twitter.zip’\n",
            "\n",
            "             full_g   0%[                    ]  21.04M   363KB/s    eta 66m 38s"
          ]
        }
      ],
      "source": [
        "!wget https://archive.org/download/full_grams_cbow_300_twitter/full_grams_cbow_300_twitter.zip\n",
        "!unzip full_grams_cbow_300_twitter.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#=============Cell 2==============\n",
        "This cell to call the necessary offices and the forms that will be required to fetch in order to execute the code correctly"
      ],
      "metadata": {
        "id": "Vs5u4RBbqkZz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlrmsR2bm4Pt"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#===============Cell 3=================\n",
        "Creat Methods:\n",
        "1- The function(Clean_and_Norm_Arabic_Text) is used to clean and normalize Arabic texts and remove distractions from them\n",
        "2- The function(get_vector) is used to find the vector to be extracted from the tokens entered into the model"
      ],
      "metadata": {
        "id": "uiId1fnytVM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def Clean_and_Norm_Arabic_Text(Arbic_text):\n",
        "    List_replace = [\"ا\",\"ا\",\"ا\",\"ه\",\" \",\" \",\"\",\"\",\"\",\" و\",\" يا\",\"\",\"\",\"\",\"ي\",\"\",' ', ' ',' ',' ? ',' ؟ ',' ! ']\n",
        "    List_search = [\"أ\",\"إ\",\"آ\",\"ة\",\"_\",\"-\",\"/\",\".\",\"،\",\" و \",\" يا \",'\"',\"ـ\",\"'\",\"ى\",\"\\\\\",'\\n', '\\t','&quot;','?','؟','!']\n",
        "  \n",
        "    p_tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
        "    Arbic_text = re.sub(p_tashkeel,\"\", Arbic_text)\n",
        "\n",
        "    p_longation = re.compile(r'(.)\\1+')\n",
        "    subst = r\"\\1\\1\"\n",
        "    Arbic_text = re.sub(p_longation, subst, Arbic_text)\n",
        "\n",
        "    Arbic_text = Arbic_text.replace('وو', 'و')\n",
        "    Arbic_text = Arbic_text.replace('يي', 'ي')\n",
        "    Arbic_text = Arbic_text.replace('اا', 'ا')\n",
        "    for i in range(0, len(List_search)):\n",
        "        Arbic_text = Arbic_text.replace(List_search[i], List_replace[i])  \n",
        "    Arbic_text = Arbic_text.strip()\n",
        "    return Arbic_text\n",
        "\n",
        "def get_vector(n_model,dim, token):\n",
        "    vec = np.zeros(dim)\n",
        "    is_vec = False\n",
        "    if token not in n_model.wv:\n",
        "        _count = 0\n",
        "        is_vec = True\n",
        "        for w in token.split(\"_\"):\n",
        "            if w in n_model.wv:\n",
        "                _count += 1\n",
        "                vec += n_model.wv[w]\n",
        "        if _count > 0:\n",
        "            vec = vec / _count\n",
        "    else:\n",
        "        vec = n_model.wv[token]\n",
        "    return vec\n"
      ],
      "metadata": {
        "id": "bBRHG2gptTU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#================Cell 4 ==================\n",
        "Load the model full_grams_cbow_300_twitter.mdl and create an object from it"
      ],
      "metadata": {
        "id": "L_bA8SNCvnf8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlMxaNY-ljVC"
      },
      "outputs": [],
      "source": [
        " model_Full_grams = gensim.models.Word2Vec.load('full_grams_cbow_300_twitter.mdl')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#==========Cell 5================\n",
        "In this cell, the set of data to be extracted properties is fetched and the property vectors are stored in a file of type Text and the Label is changed so that the positive equals (1) and the negative equals (0) and save this file for use in the classification process later"
      ],
      "metadata": {
        "id": "gz8IaUB2wcsS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioxbp5PUm2rm"
      },
      "outputs": [],
      "source": [
        "df = pd.read_excel(\"Clean_Tweets-Clean_Tweets.xlsx\")#import the Dataset \n",
        "j=0;\n",
        "FileData=open('word2vce.txt','w')#open File with name word2vce   for stor the vector it he,\n",
        "for i in df[\"text\"]:\n",
        "    token=Clean_and_Norm_Arabic_Text(i).replace(\", \", \"_\")# call the cleaning function\n",
        "    token=token.strip('[')\n",
        "    token=token.strip(']')\n",
        "    vec=get_vector(model_Full_grams,300,token)# call the get_vector function\n",
        "    for ve in vec:# stor the vector to file \n",
        "        FileData.write(repr(ve)+',')\n",
        "    if (df[\"lable\"][j]==\"positive\"):#chack a label  and change \n",
        "      FileData.write(repr(1)+'\\n')\n",
        "    else:\n",
        "      FileData.write(repr(0)+'\\n')\n",
        "    j=j+1\n",
        "FileData.close();# close the File\n",
        "      \n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#================Cell 6=================\n",
        "In this cell, an object is created from the logistic regression model, which is one of the classification models, and the process of training the model is started, and display score to training data and testing data "
      ],
      "metadata": {
        "id": "JEgTsXWMy4KM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4k7AGhNqJn3w"
      },
      "outputs": [],
      "source": [
        "def Func_Classification(PathFile):\n",
        "  path=os.path.join(PathFile)\n",
        "  data=pd.read_table(path,delimiter=',',header=None)\n",
        "  X=data.drop(300,axis=1)\n",
        "  Y=data[300]# this is Lable\n",
        "  x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=1)\n",
        "  obj_logist_Reg=LogisticRegression()#creat object form Logistic Regression \n",
        "  obj_logist_Reg.fit(x_train,y_train)#call training Function \n",
        "  print(\"the score to Training data is:\",obj_logist_Reg.score(x_train,y_train))\n",
        "  print(\"the score to Testing data is:\",obj_logist_Reg.score(x_test,y_test))\n",
        "  return obj_logist_Reg"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#============Cell 7==================\n",
        "Call Function To Classification "
      ],
      "metadata": {
        "id": "T9itwlqk5taC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YXWqYJg8FY_",
        "outputId": "d7de8b7d-c982-42c8-81a2-3b8b3b25b7b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============Score is:==============\n",
            "the score to Training data is: 0.9345794392523364\n",
            "the score to Testing data is: 0.7222222222222222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        }
      ],
      "source": [
        "print(\"==============Score is:==============\")\n",
        "obj_logist_Reg=Func_Classification('word2vce.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#=======================Cell 8================\n",
        "--------------Function predict------------- "
      ],
      "metadata": {
        "id": "kY6KnOoy6ulg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Func_predict(Text,obj_Logistc_Reg):\n",
        "    token=Clean_and_Norm_Arabic_Text(Text).replace(\", \", \"_\")# call the cleaning function\n",
        "    token=token.strip('[')\n",
        "    token=token.strip(']')\n",
        "    print(token)\n",
        "    vec=get_vector(model_Full_grams,300,token)# call the get_vector function\n",
        "    Y_Prd=obj_Logistc_Reg.predict([vec]);\n",
        "    Result=[\"positive\" if Y==1 else \"negative\" for Y in Y_Prd]\n",
        "    return Result"
      ],
      "metadata": {
        "id": "668z4kCH6JXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#===================Cell 9=================\n",
        "Call the Function predict"
      ],
      "metadata": {
        "id": "zQoWffG0AJjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(\"Clean_Tweets-Clean_Tweets.xlsx\")\n",
        "print(Func_predict(df[\"text\"][1],obj_logist_Reg))\n",
        "print(Func_predict(df[\"text\"][25],obj_logist_Reg))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XOpHIw46sdF",
        "outputId": "beaf699f-7c27-4c3a-fb6f-ea9b7810230f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "اخذت_الجرعه_الثالثه_الصباح_ونمت\n",
            "['positive']\n",
            "اخذت_الجرعه_الثالثه_ورجعت_نمت_صحيت_احسني_مختلفه_كاني_انا_يارب_تكون_بدايه_التحول_الي_اطمح_سنيني_كلها\n",
            "['negative']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"===============End================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Mkrp7UIB18P",
        "outputId": "6efb6b6a-48d7-4c22-b1f0-c3dd0721f626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===============End================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "project_Arabic_Words_Embedding.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}